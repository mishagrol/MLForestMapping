{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e535d2c",
   "metadata": {},
   "source": [
    "# 1. Forest Classification and Model Selection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore forest classification using ensemble methods and metric models. We will also perform model selection to identify the best-performing model based on various evaluation metrics.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Data Preparation**: Load and preprocess the dataset.\n",
    "2. **Model Training**: Train Random Forest and Extra Trees classifiers.\n",
    "3. **Model Evaluation**: Evaluate the performance of each model using metrics such as accuracy, precision, recall, and F1-score.\n",
    "4. **Hyperparameter Tuning**: Use techniques like Grid Search and Random Search to optimize model parameters.\n",
    "5. **Model Selection**: Compare the models and select the best one based on evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89263973-344c-460d-a588-cce007e6f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip3 -q install yellowbrick\n",
    "# !pip3 -q install imblearn\n",
    "# !pip3 -q install scienceplots\n",
    "# !pip3 -q install xgboost\n",
    "# !pip3 -q install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859a92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2102996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "#clustering\n",
    "from shapely import affinity\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#test/train split and hyperparameters optimisation\n",
    "import\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "#ML\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, recall_score, cohen_kappa_score \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm import tqdm\n",
    "\n",
    "#xgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#stats\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dict_normal_names={7: \"Pine\", \n",
    "        5:\"Aspen\",\n",
    "        4: \"Larch\", \n",
    "        1:\"Birch\",\n",
    "        6:\"Silver fir\",\n",
    "        15:\"Burnt forest\", \n",
    "        13:'Deforestation', \n",
    "        14:'Grass',\n",
    "        12:'Soil', \n",
    "        16:'Swamp', \n",
    "        11:'Water body',\n",
    "        17:'Settlements'}\n",
    "\n",
    "colors =[\n",
    "    '#117733',\n",
    "    '#50CE57',\n",
    "    '#23A28F',\n",
    "    '#5BD0AE',\n",
    "    '#88CCEE', \n",
    "    '#92462D', \n",
    "    '#DE7486',\n",
    "    '#DDCC77',\n",
    "    '#AA4499',\n",
    "    '#0f62fe',\n",
    "    '#be95ff'\n",
    "]\n",
    "\n",
    "#model saving\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb0e8d",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08403f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models():\n",
    "    return { \n",
    "        \"kNN\": KNeighborsClassifier(n_jobs=6), \n",
    "        \"SVC\": LinearSVC(), # to speed up LinearSVC\n",
    "        \"RandomForest\": RandomForestClassifier(bootstrap=True, n_jobs=6), \n",
    "        \"XGB\": xgb.XGBClassifier(n_jobs=6)}\n",
    "\n",
    "\n",
    "def get_predictions(data,\n",
    "                    model,\n",
    "                    param_grid,\n",
    "                    target_column: str = 'class',\n",
    "                    stratify_column: str = 'key',\n",
    "                    to_remove_columns: list = ['key'],\n",
    "                    test_size: float=0.3,\n",
    "                    smote_balance: bool=True,\n",
    "                    cv: int=5,\n",
    "                    n_iter_search: int=15,\n",
    "                    label_encoder: bool=False,\n",
    "                   verbose: int = 0,):\n",
    "    #test/train spliting considering key overlap problems and missed classes\n",
    "    n = 0\n",
    "    while True:\n",
    "        train_inds, test_inds = next(\n",
    "            GroupShuffleSplit(\n",
    "                test_size=test_size, n_splits=2  # ,random_state = 40\n",
    "            ).split(data, groups=data[stratify_column])\n",
    "        )\n",
    "        # because we need pixels from same plots to be separated in train and test\n",
    "\n",
    "        train = data.iloc[train_inds]\n",
    "        test = data.iloc[test_inds]\n",
    "        train_classes = train[target_column].nunique()\n",
    "        test_classes = test[target_column].nunique()\n",
    "        all_classes = data[target_column].nunique()\n",
    "        # because we need classes to be represented in train and test\n",
    "        n+=1\n",
    "        if train_classes == test_classes == all_classes:\n",
    "            break\n",
    "        if n>40:\n",
    "            print(f'N - {n}')\n",
    "            msg= f'Train - {train_classes}, Test = {test_classes}, All - {all_classes}'\n",
    "            print(msg)\n",
    "            raise KeyError('Problems in train/test split')\n",
    "        \n",
    "    train = train.drop(columns=to_remove_columns)\n",
    "    test = test.drop(columns=to_remove_columns)\n",
    "    #class balansing with smote\n",
    "    if smote_balance is True:\n",
    "        smote = SMOTE(random_state = 42)\n",
    "        X, y = smote.fit_resample(train.loc[:, train.columns!=target_column],\n",
    "                                  train[target_column]) #drops 3 columns: key, class, and forest\n",
    "        df_smote = pd.DataFrame(X, columns = train.loc[:, train.columns!=target_column].columns.tolist()) #drops 3 columns: key, class, and forest\n",
    "\n",
    "        #we set train/test from SMOTE results\n",
    "        X_train = df_smote\n",
    "        y_train = y\n",
    "        X_test = test.loc[:, train.columns!=target_column]\n",
    "        y_test = test[target_column]\n",
    "        #we set train/test as it is\n",
    "    else:\n",
    "        X_train = train.loc[:, train.columns!=target_column]\n",
    "        y_train = train[target_column]\n",
    "        X_test = test.loc[:, train.columns!=target_column]\n",
    "        y_test = test[target_column]\n",
    "    model = init_models()[model]\n",
    "    gs = RandomizedSearchCV(estimator=model,\n",
    "                            param_distributions = param_grid,\n",
    "                            n_iter = n_iter_search,\n",
    "                            cv = cv,\n",
    "                            scoring= 'f1_weighted', \n",
    "                            verbose=verbose,\n",
    "                           n_jobs = N_JOBS_CV)\n",
    "\n",
    "    if label_encoder == True:\n",
    "        le = LabelEncoder()\n",
    "        gs.fit(X_train, le.fit_transform(y_train))\n",
    "        y_pred = gs.best_estimator_.predict(X_test)\n",
    "        model_fit = gs.best_estimator_\n",
    "\n",
    "        results = {'model': model_fit,\n",
    "            'X_train data': X_train,\n",
    "            'y train data': y_train,\n",
    "            'X test data': X_test,\n",
    "            'y test data': y_test,\n",
    "            'y predicted': le.inverse_transform(y_pred)\n",
    "            }\n",
    "\n",
    "    else:\n",
    "        gs.fit(X_train, y_train)\n",
    "        y_pred = gs.best_estimator_.predict(X_test)\n",
    "        model_fit = gs.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "        results = {'model': model_fit,\n",
    "                   'X_train data': X_train,\n",
    "                   'y train data':  y_train,\n",
    "                   'X test data': X_test,\n",
    "                   'y test data': y_test,\n",
    "                   'y predicted': y_pred\n",
    "\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561af040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_description(y_true, y_pred, \n",
    "                        metrics_by_class: bool=True, \n",
    "                        confusion_matrix_multiclass_on: bool=True,\n",
    "                        binary_matrix_on: bool=False):\n",
    "\n",
    "    \n",
    "    print('Accuracy score: %.2f%%' %(accuracy_score(y_true, y_pred)*100))  \n",
    "    print('Precision score: %.2f%%' % (precision_score(y_true, y_pred, average= 'weighted')*100))\n",
    "    print('Recall score: %.2f%%' % (recall_score(y_true, y_pred, average= 'weighted')*100))\n",
    "    print('F1-Score: %.2f%%'%(f1_score(y_true, y_pred, average = 'weighted')*100))\n",
    "    print('Kappa score: %.2f%%'%(cohen_kappa_score(y_true, y_pred)*100))\n",
    "    \n",
    "    \n",
    "    #dataframe with metrics by class\n",
    "    if metrics_by_class is True:\n",
    "        metrics_by_class = pd.DataFrame(\n",
    "                {\n",
    "                    'names': list(map(dict_normal_names.get, list(np.unique(y_true)))),\n",
    "                    'f1_scores': f1_score(y_true, y_pred,\n",
    "                               average=None).round(2).tolist(),\n",
    "                    'precision': precision_score(y_true, y_pred, \n",
    "                                       average=None).round(2).tolist(),\n",
    "                    'recall':recall_score(y_true, y_pred,\n",
    "                                       average=None).round(2).tolist()\n",
    "                }\n",
    "            )\n",
    "        display(metrics_by_class)\n",
    "\n",
    "    #confusion matrix multiclass\n",
    "    if confusion_matrix_multiclass_on is True:\n",
    "        data = confusion_matrix(y_true, y_pred)\n",
    "        df_cm = pd.DataFrame(data, columns=list(map(dict_normal_names.get, list(np.unique(y_true)))), \n",
    "                             index = list(map(dict_normal_names.get, list(np.unique(y_true)))))\n",
    "        df_cm.index.name = 'Actual'\n",
    "        df_cm.columns.name = 'Predicted'\n",
    "\n",
    "        #confusion matrix plot\n",
    "        f, ax = plt.subplots(figsize=(6, 10))\n",
    "        cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "\n",
    "        sns.heatmap(df_cm, cbar=False, annot=True, cmap=cmap, square=True, fmt='.0f',\n",
    "                    annot_kws={'size': 10})\n",
    "        plt.title('Actuals vs Predicted')\n",
    "        plt.show()\n",
    "        \n",
    "    #confusion matrix binary    \n",
    "    if binary_matrix_on is True:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print('Confusion matrix\\n\\n', cm)\n",
    "        ConfusionMatrixDisplay(confusion_matrix=cm).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting dataset with metrics by class for each random prediction\n",
    "def get_classes_metrics(models_vector): #vector with model variations, y predicted and y true from the dataset\n",
    "    class_metrics_dataframe = pd.DataFrame()\n",
    "    count = 0 #counter of iteration\n",
    "\n",
    "    for i in models_vector:\n",
    "\n",
    "        count += 1 #counting\n",
    "        pred = i['y predicted'] #predicted values \n",
    "        true = i['y test data'] #corresponding labels from random test set\n",
    "        names_list = list(np.unique(true))\n",
    "\n",
    "        temp = pd.DataFrame(\n",
    "            {\n",
    "                'iteration':[count]*len(names_list), \n",
    "                'names': list(map(dict_normal_names.get, names_list)),\n",
    "                'f1_scores': f1_score(true, pred,\n",
    "                           average=None).round(2).tolist(),\n",
    "                'precision_list': precision_score(true, \n",
    "                                   pred, \n",
    "                                   average=None).round(2).tolist(),\n",
    "                'recall':recall_score(true, \n",
    "                                   pred, \n",
    "                                   average=None).round(2).tolist()\n",
    "            }\n",
    "        ) #dataset for each model \n",
    "\n",
    "        class_metrics_dataframe = pd.concat([class_metrics_dataframe, temp], ignore_index=True)\n",
    "    return class_metrics_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e923ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_average(models_vector): #vector with model variations, y predicted and y true from the dataset\n",
    "    average_metrics_dataframe = pd.DataFrame()\n",
    "    count = 0 #counter of iteration\n",
    "\n",
    "    for i in models_vector:\n",
    "\n",
    "        count += 1 #counting\n",
    "        pred = i['y predicted'] #predicted values \n",
    "        true = i['y test data'] #corresponding labels from random test set\n",
    "\n",
    "        temp = pd.DataFrame(\n",
    "            {\n",
    "                'iteration':[count],#*len(names_list), \n",
    "                #'names': list(map(dict_normal_names.get, names_list)),\n",
    "                'f1_scores': f1_score(true, pred,\n",
    "                           average='weighted').round(2).tolist(),\n",
    "                'precision_list': precision_score(true, \n",
    "                                   pred, \n",
    "                                   average='weighted').round(2).tolist(),\n",
    "                'recall':recall_score(true, \n",
    "                                   pred, \n",
    "                                   average='weighted').round(2).tolist()\n",
    "            }\n",
    "        ) #dataset for each model \n",
    "\n",
    "        average_metrics_dataframe = pd.concat([average_metrics_dataframe, temp], \n",
    "                                              ignore_index=True)\n",
    "    return average_metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(datavector_models):\n",
    "    number = get_metrics_average(datavector_models).sort_values(by='f1_scores', \n",
    "                                                                ascending=False).head(1).reset_index()['index'].values[0]\n",
    "    best_model = datavector_models[number]['model']\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfe58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_model(datavector_models):\n",
    "    number = get_metrics_average(datavector_models).sort_values(by='f1_scores', \n",
    "                                                                ascending=False).tail(1).reset_index()['index'].values[0]\n",
    "    best_model = datavector_models[number]['model']\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d9a42",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_data(path:str, cols_remove:list=['key', 'class']):\n",
    "    df = pd.read_csv(path)\n",
    "    mask = df['class']!=2\n",
    "    df=df.loc[mask]\n",
    "    x = df.drop(columns=cols_remove).values\n",
    "    #minmax scaling\n",
    "    min_max_scaler = preprocessing.MinMaxScaler() \n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    target_cols = [col for col in list(df.columns) if col not in cols_remove]\n",
    "    df.loc[:, target_cols] = x_scaled\n",
    "    return df, min_max_scaler\n",
    "folder = '../shape_data/filtered_datasets_2024/'\n",
    "df_scaled, min_max_scaler = get_scaled_data(os.path.join(folder, 'df3_notfiltered.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76851acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest():\n",
    "    n_estimators = np.arange(100,200,20)\n",
    "    max_depth = np.arange(10,110,11)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf}\n",
    "    return {\"grid\":random_grid}\n",
    "\n",
    "def get_svm():\n",
    "    svc_grid = {'C': [0.01, 0.1, 10,100], \n",
    "              # 'gamma': [1, 0.1, 0.01],\n",
    "              # 'kernel': ['rbf', 'linear', 'poly']\n",
    "             }\n",
    "    return {\"grid\":svc_grid}\n",
    "def get_KNN():\n",
    "    metric = ['euclidean','manhattan','chebyshev','minkowski']\n",
    "    n_neighbors = np.arange(4,15,2)\n",
    "    weights = ['uniform','distance']\n",
    "    random_grid_knn = {'n_neighbors': n_neighbors,\n",
    "        'weights': weights,\n",
    "        'metric': metric}\n",
    "    return {\"grid\":random_grid_knn}\n",
    "\n",
    "def get_XGB():\n",
    "    params = { 'max_depth': [3,6,10],\n",
    "           \"min_child_weight\": [0.5, 1, 2],\n",
    "           'n_estimators': np.arange(10,80,20),\n",
    "           'colsample_bytree': [0.3, 0.7, 1]}\n",
    "    return {\"grid\":params}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852eab5f",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loop(df_forest: pd.DataFrame, model: str,  param_grid: dict, smote_balance: bool, verbose: int = 0):\n",
    "    datavector = []\n",
    "    for i in range(15):\n",
    "        print(f\"{model} ---- {i}\")\n",
    "        trained_model = get_predictions(data = df_forest,\n",
    "                    model = model,\n",
    "                    param_grid = param_grid,\n",
    "                    target_column = 'class',\n",
    "                    to_remove_columns=['key'],\n",
    "                    smote_balance=smote_balance,\n",
    "                    cv=5, \n",
    "                    n_iter_search=20, \n",
    "                    label_encoder=True if model == 'XGB' else False, \n",
    "                    verbose=verbose)\n",
    "        datavector.append(trained_model)\n",
    "    return datavector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac46b2-0826-4d09-92cd-89691bb7cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_computed_metrics(metrics: pd.DataFrame, dataset: str, model: str):\n",
    "    if 'fname' not in metrics:\n",
    "        return False\n",
    "    mask = (metrics['fname']==dataset) & (metrics['model']==model) \n",
    "    if len(metrics.loc[mask])==150:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fd9a6-3395-4578-968b-19e8a7a8bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cached metrics\n"
     ]
    }
   ],
   "source": [
    "metric_fname = '../shape_data/metric_results_v7.csv'\n",
    "metric_stats_fname = '../shape_data/metric_stats_v7.csv'\n",
    "if os.path.isfile(metric_fname):\n",
    "    print('Use cached metrics')\n",
    "    metric_container = pd.read_csv(metric_fname, index_col=0)\n",
    "    metric_stats_container = pd.read_csv(metric_stats_fname, index_col=0)\n",
    "else:\n",
    "    metric_container = pd.DataFrame()\n",
    "    metric_stats_container = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b928e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../shape_data/filtered_datasets_2024/'\n",
    "datasets = os.listdir(folder)\n",
    "problems = []\n",
    "\n",
    "for dataset in sorted(datasets):\n",
    "    print(dataset)\n",
    "    scale = dataset[2]\n",
    "    df_scaled, min_max_scaler = get_scaled_data(os.path.join(folder, dataset))    \n",
    "    mask_forest = df_scaled['class']<10\n",
    "    df_forest = df_scaled.loc[mask_forest]\n",
    "    models = {\n",
    "            'RandomForest': get_random_forest(), \n",
    "             \"SVC\":get_svm(),\n",
    "            \"kNN\":get_KNN(),\n",
    "            \"XGB\":get_XGB()\n",
    "             }\n",
    "    for model, param_grid in models.items():\n",
    "        if model=='SVC':\n",
    "            N_JOBS_CV = 4\n",
    "        else:\n",
    "            N_JOBS_CV = 1\n",
    "        status_computed = check_computed_metrics(metric_container,dataset,model)\n",
    "        if status_computed==True:\n",
    "            print(f'Computed prev metrics for: {dataset} & {model}')\n",
    "            continue\n",
    "        for smote_balance in [True, False]:            \n",
    "            print(model,scale, 'SMOTE', smote_balance)\n",
    "            datavector = model_loop(df_forest, \n",
    "                                    model, \n",
    "                                    param_grid['grid'], \n",
    "                                    smote_balance, \n",
    "                                    verbose = 0)\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "            # Metrics related to forest types\n",
    "\n",
    "            model_metrics = get_classes_metrics(datavector)\n",
    "            model_metrics['model'] = model\n",
    "            model_metrics['smote_balance'] = smote_balance\n",
    "            model_metrics['scale'] = scale\n",
    "            model_metrics['fname'] = dataset\n",
    "            model_metrics['experiment_status'] = 'Done'\n",
    "            metric_container = pd.concat([metric_container, model_metrics], axis=0)\n",
    "\n",
    "            # Metrics related to forest\n",
    "            metricts_stats = get_metrics_average(datavector)\n",
    "            metricts_stats['model'] = model\n",
    "            metricts_stats['smote_balance'] = smote_balance\n",
    "            metricts_stats['scale'] = scale\n",
    "            metricts_stats['fname'] = dataset\n",
    "            metricts_stats['experiment_status'] = 'Done'\n",
    "            metric_stats_container = pd.concat([metric_stats_container, metricts_stats], axis=0)\n",
    "            \n",
    "            best_model = get_best_model(datavector)\n",
    "            core = dataset.split('.')[0]\n",
    "            model_path = os.path.join(f'../models/best_models/{model}_{core}.joblib')\n",
    "            dump(best_model, model_path)\n",
    "        metric_container.to_csv('../shape_data/metric_results_v7.csv')\n",
    "        metric_stats_container.to_csv('../shape_data/metric_stats_v7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72203efb",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65edba1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
